{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ca4c88",
   "metadata": {},
   "source": [
    "# Tópico 1 - Diferenças entre o AdaBoost e o GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6db1f3",
   "metadata": {},
   "source": [
    "O AdaBoost (Adaptive Boosting) e o GBM (Gradient Boosting Machine) são dois algoritmos populares de aprendizado de máquina que pertencem à família de algoritmos de boosting. Embora ambos sejam usados para melhorar a precisão de modelos de aprendizado de máquina fracos, existem algumas diferenças entre eles. Aqui estão cinco diferenças entre o AdaBoost e o GBM:\n",
    "\n",
    "### 1. Abordagem de otimização:\n",
    "- **AdaBoost:** O AdaBoost otimiza a função de perda minimizando o erro ponderado nas instâncias de treinamento. A cada iteração, o algoritmo ajusta o peso de cada instância, dando maior importância às instâncias mal classificadas anteriormente.\n",
    "- **GBM:** O GBM otimiza a função de perda minimizando o erro residual. A cada iteração, o algoritmo ajusta o modelo para se concentrar nos resíduos (diferença entre a resposta real e a resposta predita) do modelo anterior.\n",
    "\n",
    "### 2. Adição de modelos fracos:\n",
    "- **AdaBoost:** O AdaBoost adiciona modelos fracos sequencialmente, atribuindo pesos a cada modelo com base em seu desempenho na iteração anterior.\n",
    "- **GBM:** O GBM adiciona modelos fracos de forma aditiva, onde cada novo modelo é ajustado para minimizar os resíduos do modelo anterior.\n",
    "\n",
    "### 3. Tipo de modelo fraco:\n",
    "- **AdaBoost:** O AdaBoost pode usar qualquer algoritmo de aprendizado de máquina fraco como base, como árvores de decisão com pouca profundidade (stumps).\n",
    "- **GBM:** O GBM usa árvores de decisão como seus modelos fracos, geralmente árvores de decisão com maior profundidade.\n",
    "\n",
    "### 4. Abordagem à diversidade:\n",
    "- **AdaBoost:** O AdaBoost busca diversidade selecionando instâncias mal classificadas e ajustando o foco nessas instâncias em iterações posteriores.\n",
    "- **GBM:** O GBM busca diversidade ajustando os resíduos do modelo anterior, permitindo que o próximo modelo se concentre em áreas onde o modelo anterior teve um desempenho ruim.\n",
    "\n",
    "### 5. Sensibilidade a outliers:\n",
    "- **AdaBoost:** O AdaBoost é mais sensível a outliers, uma vez que atribui maior peso a instâncias mal classificadas, o que pode resultar em um ajuste excessivo dessas instâncias.\n",
    "- **GBM:** O GBM é menos sensível a outliers, pois os resíduos são tratados de forma aditiva, o que reduz o impacto de instâncias individuais no processo de aprendizado.\n",
    "\n",
    "Essas são algumas das diferenças entre o AdaBoost e o GBM em termos de abordagem de otimização, adição de modelos fracos, tipo de modelo fraco, diversidade e sensibilidade a outliers. É importante lembrar que essas diferenças podem variar dependendo das implementações específicas e configurações dos algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5d434",
   "metadata": {},
   "source": [
    "# Tópico 2 - Exemplos de classificação e de refressão do GBM\n",
    "\n",
    "Exemplos de códigos com demonstração de classificação e de regressão do GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b8ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão do modelo GBM de classificação: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de classificação com o GBM\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando e treinando o modelo de classificação GBM\n",
    "gbm_classifier = GradientBoostingClassifier()\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbm_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisão do modelo GBM de classificação: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc5f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro médio quadrático do modelo GBM de regressão: 0.29\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de regressão com o GBM\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Carregando o conjunto de dados California housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando e treinando o modelo de regressão GBM\n",
    "gbm_regressor = GradientBoostingRegressor()\n",
    "gbm_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbm_regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Erro médio quadrático do modelo GBM de regressão: {:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856846dd",
   "metadata": {},
   "source": [
    "# Tópico 3 - Cinco hiperparâmetros importantes no GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584d39c",
   "metadata": {},
   "source": [
    "O GBM (Gradient Boosting Machine) é um algoritmo flexível com vários hiperparâmetros que podem ser ajustados para otimizar o desempenho do modelo. Aqui estão cinco hiperparâmetros importantes no GBM:\n",
    "\n",
    "### n_estimators\n",
    "Este hiperparâmetro controla o número de árvores de decisão individuais a serem construídas no GBM. Um número maior de estimadores geralmente aumenta a capacidade do modelo, mas também aumenta o tempo de treinamento. É importante ajustar esse hiperparâmetro para encontrar um equilíbrio entre desempenho e eficiência.\n",
    "\n",
    "### learning_rate\n",
    "O learning rate controla a contribuição de cada árvore de decisão para a atualização do modelo. Um learning rate menor significa que cada árvore contribuirá menos para o modelo final, tornando o processo de ajuste mais lento, mas também mais preciso. Por outro lado, um learning rate maior acelera o processo de ajuste, mas pode levar a um ajuste excessivo (overfitting) se for muito alto. É necessário ajustar o learning rate em conjunto com o número de estimadores para otimizar o desempenho do GBM.\n",
    "\n",
    "### max_depth\n",
    "Esse hiperparâmetro controla a profundidade máxima de cada árvore de decisão no GBM. Uma árvore mais profunda pode capturar relações mais complexas nos dados, mas também aumenta o risco de sobreajuste. Limitar a profundidade máxima é uma forma de controlar a complexidade do modelo e evitar o ajuste excessivo.\n",
    "\n",
    "### subsample\n",
    "O subsample determina a fração de amostras que são usadas para ajustar cada árvore de decisão. Um valor menor de subsample implica em um conjunto de treinamento menor para cada árvore, o que pode aumentar a variância do modelo, mas também pode melhorar a generalização. Um valor comum é 0.8, o que significa que cada árvore é ajustada em uma amostra aleatória contendo 80% das observações originais.\n",
    "\n",
    "### min_samples_split\n",
    "Esse hiperparâmetro controla o número mínimo de amostras necessárias para dividir um nó interno em uma árvore de decisão. Um valor maior de min_samples_split reduz a probabilidade de divisões excessivamente complexas que podem levar ao sobreajuste. É importante ajustar esse hiperparâmetro para evitar o ajuste excessivo e permitir que o modelo generalize bem.\n",
    "\n",
    "Esses são apenas alguns dos hiperparâmetros importantes no GBM. Existem outros hiperparâmetros, como **`min_samples_leaf`**, **`max_features`**, **`subsample`**, entre outros, que também podem ter um impacto significativo no desempenho do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4d254",
   "metadata": {},
   "source": [
    "# Tópico 4 - Encontrando os melhores hiperparâmetros para o conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616bc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Carregando o conjunto de dados California housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definindo os hiperparâmetros a serem testados\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 4]\n",
    "}\n",
    "\n",
    "gbm_regressor = GradientBoostingRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(gbm_regressor, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Erro médio quadrático do melhor modelo GBM de regressão: {:.2f}\".format(mse))\n",
    "print(\"Melhores hiperparâmetros encontrados:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4b401",
   "metadata": {},
   "source": [
    "Neste exemplo, estamos utilizando o **`GridSearchCV`** do Scikit-learn para realizar uma busca em grade nos hiperparâmetros definidos no **`param_grid`**. O GridSearchCV realiza uma validação cruzada com 3 folds para avaliar o desempenho de cada combinação de hiperparâmetros.\n",
    "\n",
    "Após o GridSearch, podemos obter os melhores hiperparâmetros através da propriedade **`best_params_`** e o melhor modelo através da propriedade **`best_estimator_`**. Em seguida, realizamos previsões no conjunto de teste utilizando o melhor modelo e calculamos o erro médio quadrático (MSE).\n",
    "\n",
    "Lembre-se de que o GridSearch pode ser computacionalmente intensivo, especialmente quando o número de combinações de hiperparâmetros é grande ou o conjunto de dados é grande. Portanto, pode levar algum tempo para executar o código acima, dependendo das configurações do seu ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d47d878",
   "metadata": {},
   "source": [
    "# Tópico 5 - Diferença entre o GBM e o Stochastic GBM de Jerome Friedman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58622056",
   "metadata": {},
   "source": [
    "O Gradient Boosting é uma técnica de aprendizado de máquina que combina vários modelos de aprendizado fracos para formar um modelo mais forte. Ele treina sucessivamente uma série de modelos de aprendizado fracos, como árvores de decisão simples, e cada modelo subsequente é treinado para corrigir os erros do modelo anterior. Essa abordagem em cascata permite que o algoritmo aprenda com os erros e melhore gradualmente suas previsões.\n",
    "\n",
    "O Gradient Boosting Machine (GBM) é uma variação do algoritmo Gradient Boosting que usa funções de perda diferenciáveis e otimização por gradiente para ajustar os parâmetros do modelo. O GBM estocástico, por outro lado, é uma extensão do GBM que introduz aleatoriedade no processo de treinamento.\n",
    "\n",
    "A maior diferença entre o GBM estocástico e o algoritmo mencionado no artigo do Jerome Friedman pode variar dependendo do contexto específico do artigo. No entanto, no contexto do Gradient Boosting, uma possível diferença é que o GBM estocástico utiliza uma técnica chamada amostragem estocástica durante o treinamento. Em vez de usar todo o conjunto de dados para treinar cada modelo fraco, o GBM estocástico amostra aleatoriamente uma porção do conjunto de dados para cada etapa do treinamento. Essa amostragem estocástica pode ajudar a reduzir o sobreajuste e melhorar a capacidade de generalização do modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
